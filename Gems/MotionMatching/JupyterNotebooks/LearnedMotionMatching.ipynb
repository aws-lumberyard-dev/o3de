{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45837174",
   "metadata": {},
   "outputs": [],
   "source": [
    "folderPath = \"D:/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e3d25",
   "metadata": {},
   "source": [
    "# Load data helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a44238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def PrintGreen(text):\n",
    "    print('\\x1b[6;30;42m' + text + '\\x1b[0m')\n",
    "    \n",
    "def PrintRed(text):\n",
    "    print('\\33[41m' + text + '\\x1b[0m')\n",
    "\n",
    "def LoadData(filename, rowsName, columnsName):\n",
    "    newDataframe = pd.read_csv(filename, na_values = 'null')\n",
    "    if newDataframe.shape[0] > 0 and newDataframe.shape[1] > 0:\n",
    "        PrintGreen(\"Loading \" + filename + \" succeeded\");\n",
    "    else:\n",
    "        PrintRed(\"Loading \" + filename + \" failed!\");\n",
    "\n",
    "    print(rowsName + \" = \" + str(newDataframe.shape[0]))\n",
    "    print(columnsName + \" = \" + str(newDataframe.shape[1]))\n",
    "\n",
    "    return newDataframe\n",
    "\n",
    "# Ensure to show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dab663",
   "metadata": {},
   "source": [
    "## Motion Database Poses\n",
    "Evaluated skeletal poses at a given sample rate. Position and rotation in local space. Rotation is represented as the X and Y basis vectors of the transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c7caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPosRotModelSpace = LoadData(folderPath + 'MotionMatchingDatabase_Poses_PosRot_ModelSpace_60Hz.csv', \"Frames\", \"PoseComponents\")\n",
    "dataPosRotModelSpace.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6be672",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRotModelSpace = LoadData(folderPath + 'MotionMatchingDatabase_Poses_Rot_ModelSpace_60Hz.csv', \"Frames\", \"PoseComponents\")\n",
    "dataRotModelSpace.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c14967",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPosRotLocalSpace = LoadData(folderPath + 'MotionMatchingDatabase_Poses_PosRot_LocalSpace_60Hz.csv', \"Frames\", \"PoseComponents\")\n",
    "dataPosRotLocalSpace.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca670cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRotLocalSpace = LoadData(folderPath + 'MotionMatchingDatabase_Poses_Rot_LocalSpace_60Hz.csv', \"Frames\", \"PoseComponents\")\n",
    "dataRotLocalSpace.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eba8ed",
   "metadata": {},
   "source": [
    "## Motion Database Features\n",
    "Feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFeatures = LoadData(folderPath + 'MotionMatchingDatabase_Features_60Hz.csv', \"Frames\", \"FeatureComponents\")\n",
    "\n",
    "if (dataPosRotLocalSpace.shape[0] == dataFeatures.shape[0]\n",
    "    and dataPosRotModelSpace.shape[0] == dataFeatures.shape[0]\n",
    "    and dataRotLocalSpace.shape[0] == dataFeatures.shape[0]\n",
    "    and dataRotModelSpace.shape[0] == dataFeatures.shape[0]):\n",
    "    PrintGreen(\"Frame numbers match.\")\n",
    "else:\n",
    "    PrintRed(\"Frame numbers do not match!\")\n",
    "\n",
    "dataFeatures.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78160101",
   "metadata": {},
   "source": [
    "## Recorded data (Poses, features and best matching frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213daccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_poses = LoadData(folderPath + 'ReallyLongRuntimeRecordings/RuntimeRecording_Poses.csv', \"Frames\", \"PoseComponents\")\n",
    "recording_rotations = LoadData(folderPath + 'ReallyLongRuntimeRecordings/RuntimeRecording_Rotations.csv', \"Frames\", \"RotationComponents\")\n",
    "recording_features = LoadData(folderPath + 'ReallyLongRuntimeRecordings/RuntimeRecording_Features.csv', \"Frames\", \"FeatureComponents\")\n",
    "recording_bestMatchFrames = LoadData(folderPath + 'ReallyLongRuntimeRecordings/RuntimeRecording_BestMatchingFrames.csv', \"Frames\", \"BestMatchingFrameComponents\")\n",
    "\n",
    "if (recording_poses.shape[0] == recording_rotations.shape[0] == recording_features.shape[0] == recording_bestMatchFrames.shape[0]):\n",
    "    PrintGreen(\"Frame numbers match.\")\n",
    "else:\n",
    "    PrintRed(\"Frame numbers do not match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce740cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "recordingPoseData = np.delete(recording_poses.values, -1, axis=0) # remove last frame\n",
    "recordingFeatureData = np.delete(recording_features.values, -1, axis=0) # remove last frame\n",
    "print(recordingPoseData.shape)\n",
    "print(recordingFeatureData.shape)\n",
    "recording_input = np.concatenate((recordingPoseData, recordingFeatureData), axis=1) # concatenate horizontally\n",
    "print(recording_input.shape)\n",
    "\n",
    "recordingOutputPoseData = np.delete(recording_rotations.values, 0, axis=0)\n",
    "print(recordingOutputPoseData.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9faeb0f",
   "metadata": {},
   "source": [
    "## Data Preparation / Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "posRotModelSpaceDataScaler = preprocessing.StandardScaler()\n",
    "dataPosRotModelSpaceScaled = posRotModelSpaceDataScaler.fit_transform(dataPosRotModelSpace.values)\n",
    "\n",
    "rotModelSpaceDataScaler = preprocessing.StandardScaler()\n",
    "dataRotModelSpaceScaled = rotModelSpaceDataScaler.fit_transform(dataRotModelSpace.values)\n",
    "\n",
    "posRotLocalSpaceDataScaler = preprocessing.StandardScaler()\n",
    "dataPosRotLocalSpaceScaled = posRotLocalSpaceDataScaler.fit_transform(dataPosRotLocalSpace.values)\n",
    "\n",
    "rotLocalSpaceDataScaler = preprocessing.StandardScaler()\n",
    "dataRotLocalSpaceScaled = rotLocalSpaceDataScaler.fit_transform(dataRotLocalSpace.values)\n",
    "\n",
    "featureDataScaler = preprocessing.StandardScaler()\n",
    "dataFeaturesScaled = featureDataScaler.fit_transform(dataFeatures.values)\n",
    "\n",
    "# Input (Pose + Features)\n",
    "inputPoseData = np.delete(dataPosRotModelSpaceScaled, -1, axis=0) # remove last frame\n",
    "inputPoseData2 = np.delete(dataPosRotLocalSpaceScaled, -1, axis=0) # remove last frame\n",
    "inputFeatureData = np.delete(dataFeaturesScaled, -1, axis=0) # remove last frame\n",
    "print(inputPoseData.shape)\n",
    "print(inputFeatureData.shape)\n",
    "data_input = np.concatenate((inputPoseData, inputFeatureData), axis=1) # concatenate horizontally\n",
    "print(data_input.shape)\n",
    "\n",
    "# Output (Next pose)\n",
    "data_output = dataRotLocalSpaceScaled\n",
    "data_output = np.delete(data_output, 0, axis=0) # remove first frame\n",
    "print(data_output.shape)\n",
    "\n",
    "# Input (Pose + Features)\n",
    "# inputPoseData = np.delete(dataPosRotLocalSpace.values, -1, axis=0) # remove last frame\n",
    "# inputFeatureData = np.delete(dataFeatures.values, -1, axis=0) # remove last frame\n",
    "# print(inputPoseData.shape)\n",
    "# print(inputFeatureData.shape)\n",
    "# data_input = np.concatenate((inputPoseData, inputFeatureData), axis=1) # concatenate horizontally\n",
    "# print(data_input.shape)\n",
    "\n",
    "# # Output (Next pose)\n",
    "# outputPoseData = np.delete(dataRotLocalSpace.values, 0, axis=0)\n",
    "# # outputFeatureData = np.delete(dataFeatures.values, 0, axis=0)\n",
    "# # print(outputPoseData.shape)\n",
    "# # print(outputFeatureData.shape)\n",
    "# # data_output = np.concatenate((outputPoseData, outputFeatureData), axis=1) # concatenate horizontally\n",
    "# data_output = outputPoseData\n",
    "# print(data_output.shape)\n",
    "\n",
    "# data_input = np.concatenate((data_input, recording_input), axis=0)\n",
    "# print(data_input.shape)\n",
    "\n",
    "# data_output = np.concatenate((data_output, recordingOutputPoseData), axis=0)\n",
    "# print(data_output.shape)\n",
    "\n",
    "# data_input = recording_input\n",
    "# print(data_input.shape)\n",
    "\n",
    "# data_output = recordingOutputPoseData\n",
    "# print(data_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff63f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def GetBadFrames(frameData):\n",
    "    sum_square_diff = np.empty(len(frameData)-1)\n",
    "    badFrames = []\n",
    "    for i in range(1, len(frameData)):\n",
    "        sum_square_diff[i-1] = np.linalg.norm((frameData[i] - frameData[i-1]))\n",
    "        if sum_square_diff[i-1] > 11:\n",
    "            badFrames.append(i-1)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(sum_square_diff)\n",
    "    plt.show\n",
    "    return badFrames\n",
    "\n",
    "inputFrameData = data_input\n",
    "badFrames = GetBadFrames(inputFrameData)\n",
    "print(len(badFrames))\n",
    "\n",
    "data_input = np.delete(data_input, badFrames, axis=0)\n",
    "data_output = np.delete(data_output, badFrames, axis=0)\n",
    "print(len(data_input))\n",
    "print(len(data_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a5501",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955f0eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert Pandas DataFrames to PyTorch tensors\n",
    "# X = torch.tensor(data_input, dtype=torch.float32)\n",
    "# y = torch.tensor(data_output, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dd4ee7",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "logdir = folderPath + \"TensorBoard/\" # + datetime.now().strftime(\"%Y%m%d\") + \"/\"\n",
    "tensorBoardWriter = SummaryWriter(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33abbcd9",
   "metadata": {},
   "source": [
    "## PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9451a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def ConstructModel(inputFeatures, outputFeatures):\n",
    "    hidden_layer = 1024\n",
    "    model = nn.Sequential(nn.Linear(inputFeatures, hidden_layer),\n",
    "                          nn.ReLU(),\n",
    "                          #nn.Dropout(0.1), # probability of an element to be zeroed. Default: 0.5\n",
    "                          #nn.BatchNorm1d(layer2),\n",
    "                          nn.Linear(hidden_layer, hidden_layer),\n",
    "                          nn.ReLU(), #nn.LeakyReLU(),\n",
    "                          #nn.Dropout(0.1),\n",
    "                          #nn.BatchNorm1d(layer1),\n",
    "                          nn.Linear(hidden_layer, outputFeatures),\n",
    "                          #nn.Sigmoid()\n",
    "                          #nn.ReLU()\n",
    "                          #nn.Tanh()\n",
    "                          )\n",
    "    \n",
    "    def init_weights(model):\n",
    "        if isinstance(model, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform(model.weight)\n",
    "            model.bias.data.fill_(0.01)\n",
    "\n",
    "    init_weights(model)\n",
    "    print(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3c37ec",
   "metadata": {},
   "source": [
    "## Export PyTorch model to an ONNX file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx\n",
    "\n",
    "def ExportModelToOnnx(model, filename, dummy_input):\n",
    "    torch.onnx.export(model,                     # model being run\n",
    "                      dummy_input,               # model input (or a tuple for multiple inputs)\n",
    "                      filename,                  # where to save the model (can be a file or file-like object)\n",
    "                      export_params=True,        # store the trained parameter weights inside the model file\n",
    "                      opset_version=10,          # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                      input_names = ['input'],   # the model's input names\n",
    "                      output_names = ['output'], # the model's output names\n",
    "                      dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                    'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a247051",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5\n",
    "epochs = 6\n",
    "loss_function = nn.MSELoss()\n",
    "mini_batch_size = 16\n",
    "# Note: Optimizer can be changed in the code below (as it needs the model parameters it can't be done here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_function(input, target):\n",
    "#     total = 0\n",
    "#     for batch_index in range(len(input)):\n",
    "#         diff = input[batch_index] - target[batch_index]\n",
    "#         for i in range(1098, 1157):\n",
    "#             diff[i] *= 10\n",
    "#         total += (diff**2).mean()\n",
    "#     return total\n",
    "\n",
    "# def loss_function(input, target):\n",
    "#     total = 0\n",
    "#     for batch_index in range(len(input)):\n",
    "#         diff = (input[batch_index] - target[batch_index])**2\n",
    "#         for i in range(1098, 1157):\n",
    "#             diff[i] *= 20\n",
    "#         total += diff.mean()\n",
    "#     return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8259154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert an input and output tensor into a dataset\n",
    "# Inputs:\n",
    "# X: torch.tensor specifying the training input data\n",
    "# y: torch.tensor specifying the training labels\n",
    "class CreatePytorchDataset(Dataset):\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx],self.y_train[idx]\n",
    "    def __init__(self,X, y):\n",
    "        self.x_train=X\n",
    "        self.y_train=y \n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "def Training(model, train_loader, test_loader, optimizer):\n",
    "\n",
    "    timeCode = datetime.now().strftime(\"%H:%M:%S\") # used for TensorBoard\n",
    "\n",
    "    # Re-train on the same data for the given amount of epochs\n",
    "    losses = []\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "        print(\"Epoch \" + str(epoch) + \": \")\n",
    "        running_loss = 0.0\n",
    "        current_mini_batch = 0\n",
    "\n",
    "        # Mini-batches\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "            # Get the inputs and labels/outputs from the dataset\n",
    "            X, y = data\n",
    "\n",
    "            #################\n",
    "            # Backpropagation\n",
    "\n",
    "            # Zero the gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform forward pass and compute prediction\n",
    "            pred_y = model(X)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(pred_y, y)\n",
    "            running_loss += loss.item()\n",
    "            tensorBoardWriter.add_scalar(timeCode + \" Loss / train\", loss.item(), current_mini_batch)\n",
    "\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            print_each = 100\n",
    "            if current_mini_batch % print_each == print_each - 1:\n",
    "                averaged_loss = running_loss / print_each\n",
    "                print(\"Loss after mini-batch %5d: %.5f\" % (current_mini_batch + 1, averaged_loss))\n",
    "                tensorBoardWriter.add_scalar(timeCode + \" Running loss / train\", averaged_loss, current_mini_batch)\n",
    "                losses.append(averaged_loss)\n",
    "                running_loss = 0.0\n",
    "\n",
    "            current_mini_batch = current_mini_batch + 1\n",
    "\n",
    "\n",
    "    lossData = pd.DataFrame(losses)\n",
    "    lossData.to_csv(\"D:/OnnxModelLosses_PosRotFeaturesWithHandAndHeadModel_To_RotLocal_Normalized_NoBadFrames_Shuffled_LR1e-4_Batch16_Hidden1024_WeightDecay1e-5_Epochs6_NormalizeDataOn_Threshold10.csv\", index=False)\n",
    "    plt.plot(losses)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('mini-batch')\n",
    "    plt.title(\"Learning rate %f\"%(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    # TensorBoard\n",
    "    tensorBoardWriter.close()\n",
    "\n",
    "\n",
    "# Convert Pandas DataFrames to PyTorch tensors\n",
    "X = torch.tensor(data_input, dtype=torch.float32)\n",
    "y = torch.tensor(data_output, dtype=torch.float32)\n",
    "\n",
    "# Construct the PyTorch dataset and data loaders\n",
    "torchDataset = CreatePytorchDataset(X, y)\n",
    "train_loader = torch.utils.data.DataLoader(torchDataset, batch_size=mini_batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torchDataset, batch_size=mini_batch_size, shuffle=True)\n",
    "\n",
    "# Construct the model and optimizer and train the model\n",
    "inputFeatures = X.shape[1]\n",
    "outputFeatures = y.shape[1]\n",
    "model = ConstructModel(inputFeatures, outputFeatures)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "Training(model, train_loader, test_loader, optimizer)\n",
    "\n",
    "# Export the trained model to an ONNX file\n",
    "filename = folderPath + 'OnnxModel_PosRotFeaturesWithHandAndHeadModel_To_RotLocal_Normalized_NoBadFrames_Shuffled_LR1e-4_Batch16_Hidden1024_WeightDecay1e-5_Epochs6_NormalizeDataOn_Threshold10.onnx'\n",
    "ExportModelToOnnx(model, filename, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d672bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"D:/OnnxModel_PosRotFeaturesWithHandAndHeadModelAndLocal_To_RotLocal_Normalized_NoBadFrames_Shuffled_LR1e-4_Batch16_Hidden1024_WeightDecay1e-5_Epochs3_NormalizeDataOn_Threshold11.onnx\")\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "# ort_input = {\"input\": [X[0].numpy()]}\n",
    "# X = torch.tensor(np.concatenate((dataPosRotLocalSpace.values, dataFeatures.values), axis=1), dtype=torch.float32)\n",
    "X = torch.tensor(data_input, dtype=torch.float32)\n",
    "ort_input = {\"input\": X.numpy()}\n",
    "ort_outs = ort_session.run(None, ort_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e8501",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_outs[0][25][25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2746a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_outs = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(torch_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180cbad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# referencePoses = np.concatenate((dataRotLocalSpace.values, dataFeatures.values), axis=1)\n",
    "referencePoses = dataRotLocalSpaceScaled\n",
    "# referencePoses = dataRotLocalSpace.values\n",
    "referencePoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f56b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "refPoses = pd.DataFrame(data_input)\n",
    "refPoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95ae7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = np.empty(len(ort_outs[0])-1)\n",
    "for i in range(len(ort_outs[0])-1):\n",
    "    l1[i] = np.linalg.norm((ort_outs[0][i] - referencePoses[i + 1]), ord=1)\n",
    "print(\"Avg L1: \" + str(np.average(l1)))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(l1)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5db3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = np.empty(len(ort_outs[0])-1)\n",
    "for i in range(len(ort_outs[0])-1):\n",
    "    l2[i] = np.linalg.norm((ort_outs[0][i] - referencePoses[i + 1]))\n",
    "print(\"Avg L2: \" + str(np.average(l2)))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(l2)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da62d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalizedOrtOuts = rotLocalSpaceDataScaler.inverse_transform(ort_outs[0])\n",
    "# referencePoses = np.concatenate((dataRotLocalSpace.values, dataFeatures.values), axis=1)\n",
    "unnormalizedReferencePoses = dataRotLocalSpace.values\n",
    "# referencePoses = dataRotLocalSpace.values\n",
    "unnormalizedReferencePoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa138add",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = np.empty(len(unnormalizedOrtOuts)-1)\n",
    "for i in range(len(unnormalizedOrtOuts)-1):\n",
    "    l1[i] = np.linalg.norm((unnormalizedOrtOuts[i] - unnormalizedReferencePoses[i + 1]), ord=1)\n",
    "print(\"Avg L1: \" + str(np.average(l1)))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(l1)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5533da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = np.empty(len(unnormalizedOrtOuts)-1)\n",
    "for i in range(len(unnormalizedOrtOuts)-1):\n",
    "    l2[i] = np.linalg.norm((unnormalizedOrtOuts[i] - unnormalizedReferencePoses[i + 1]))\n",
    "print(\"Avg L2: \" + str(np.average(l2)))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(l2)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168880f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalPoses = pd.DataFrame(unnormalizedOrtOuts, columns=dataRotLocalSpace.columns.values)\n",
    "finalPoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e232d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalPoses.to_csv(\"D:/InferencedPoses_PosRotFeaturesWithHandAndHeadModelAndLocal_To_RotLocal_Normalized_NoBadFrames_Shuffled_LR1e-4_Batch16_Hidden1024_WeightDecay1e-5_Epochs3_NormalizeDataOn_Threshold11_UNNORMALIZED.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0bc409",
   "metadata": {},
   "source": [
    "Test 1:\n",
    "\n",
    "result_data\n",
    "\n",
    "iterate through data_input\n",
    "    resulting_scaled_pose = model.predict(data_input[i])\n",
    "    resulting_pose = poseDataScaler.inverse_transform(resulting_scaled_pose)\n",
    "    result_data.add_row(resulting_pose)\n",
    "\n",
    "store_to_csv(result_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Result: csv file hopefully containing a similar result as the original animation\n",
    "\n",
    "* New AnimGraphNode that can read a csv file and play it back (by applying the next pose with each Update())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TODO:\n",
    "* Infer the model on all input frames, store the results inside a new .CSV file\n",
    "* Create a way to play back the \"animation\" of the exported CSV file\n",
    "* (If that looks a bit OK), use ONNX in motion matching code to step forward\n",
    "* motion borders are an issue. inside the motion database we have several individual motions, but we don't separate them yet here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "64e2c40b1e2e5187dbc581262b84e05636cbc152dfd1c72af75c368d3284d05b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
