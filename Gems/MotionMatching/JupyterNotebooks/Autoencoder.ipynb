{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderPath = \"D:/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def PrintGreen(text):\n",
    "    print('\\x1b[6;30;42m' + text + '\\x1b[0m')\n",
    "    \n",
    "def PrintRed(text):\n",
    "    print('\\33[41m' + text + '\\x1b[0m')\n",
    "\n",
    "def LoadData(filename, rowsName, columnsName):\n",
    "    newDataframe = pd.read_csv(filename, na_values = 'null')\n",
    "    if newDataframe.shape[0] > 0 and newDataframe.shape[1] > 0:\n",
    "        PrintGreen(\"Loading \" + filename + \" succeeded\");\n",
    "    else:\n",
    "        PrintRed(\"Loading \" + filename + \" failed!\");\n",
    "\n",
    "    print(rowsName + \" = \" + str(newDataframe.shape[0]))\n",
    "    print(columnsName + \" = \" + str(newDataframe.shape[1]))\n",
    "\n",
    "    return newDataframe\n",
    "\n",
    "# Ensure to show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPosRotModelSpace = LoadData(folderPath + 'MotionMatchingDatabase_Poses_PosRot_ModelSpace_60Hz_Training.csv', \"Frames\", \"PoseComponents\")\n",
    "dataPosRotModelSpace.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPosRotModelSpaceTesting = LoadData(folderPath + 'MotionMatchingDatabase_Poses_PosRot_ModelSpace_60Hz_Testing.csv', \"Frames\", \"PoseComponents\")\n",
    "dataPosRotModelSpaceTesting.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "posRotModelSpaceDataScaler = preprocessing.StandardScaler()\n",
    "dataPosRotModelSpaceScaled = posRotModelSpaceDataScaler.fit_transform(dataPosRotModelSpace.values)\n",
    "\n",
    "posRotModelSpaceDataScalerTesting = preprocessing.StandardScaler()\n",
    "posRotModelSpaceDataScalerTesting.fit(dataPosRotModelSpace.values)\n",
    "dataPosRotModelSpaceScaledTesting = posRotModelSpaceDataScalerTesting.transform(dataPosRotModelSpaceTesting.values)\n",
    "\n",
    "# Input (Pose + Features)\n",
    "data_input = dataPosRotModelSpaceScaled\n",
    "print(data_input.shape)\n",
    "\n",
    "# Output (Next pose)\n",
    "data_output = dataPosRotModelSpaceScaled\n",
    "print(data_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "logdir = folderPath + \"TensorBoard/\" # + datetime.now().strftime(\"%Y%m%d\") + \"/\"\n",
    "tensorBoardWriter = SummaryWriter(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def ConstructModelEncoder(inputFeatures, outputFeatures):\n",
    "    hidden_layer1 = 936\n",
    "    hidden_layer2 = 468\n",
    "    hidden_layer3 = 234\n",
    "    hidden_layer4 = 117\n",
    "    hidden_layer5 = 78\n",
    "    hidden_layer6 = 39\n",
    "\n",
    "    hidden_layer_v2_1 = 624\n",
    "    hidden_layer_v2_2 = 312\n",
    "    hidden_layer_v2_3 = 156\n",
    "    hidden_layer_v2_4 = 78\n",
    "    model = nn.Sequential(nn.Linear(inputFeatures, hidden_layer4),\n",
    "                          nn.ReLU()\n",
    "                          )\n",
    "    \n",
    "    def init_weights(model):\n",
    "        if isinstance(model, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform(model.weight)\n",
    "            model.bias.data.fill_(0.01)\n",
    "\n",
    "    init_weights(model)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "def ConstructModelDecoder(inputFeatures, outputFeatures):\n",
    "    hidden_layer1 = 936\n",
    "    hidden_layer2 = 468\n",
    "    hidden_layer3 = 234\n",
    "    hidden_layer4 = 117\n",
    "    hidden_layer5 = 78\n",
    "    hidden_layer6 = 39\n",
    "\n",
    "    hidden_layer_v2_1 = 624\n",
    "    hidden_layer_v2_2 = 312\n",
    "    hidden_layer_v2_3 = 156\n",
    "    hidden_layer_v2_4 = 78\n",
    "    model = nn.Sequential(nn.Linear(hidden_layer4, outputFeatures))\n",
    "    \n",
    "    def init_weights(model):\n",
    "        if isinstance(model, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform(model.weight)\n",
    "            model.bias.data.fill_(0.01)\n",
    "\n",
    "    init_weights(model)\n",
    "    print(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 128\n",
    "epochs = 100\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert an input and output tensor into a dataset\n",
    "# Inputs:\n",
    "# X: torch.tensor specifying the training input data\n",
    "# y: torch.tensor specifying the training labels\n",
    "class CreatePytorchDataset(Dataset):\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx],self.y_train[idx]\n",
    "    def __init__(self,X, y):\n",
    "        self.x_train=X\n",
    "        self.y_train=y \n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "# Convert Pandas DataFrames to PyTorch tensors\n",
    "X = torch.tensor(data_input, dtype=torch.float32)\n",
    "y = torch.tensor(data_output, dtype=torch.float32)\n",
    "\n",
    "# Convert Pandas DataFrames to PyTorch tensors\n",
    "Xt = torch.tensor(dataPosRotModelSpaceScaledTesting, dtype=torch.float32)\n",
    "yt = torch.tensor(dataPosRotModelSpaceScaledTesting, dtype=torch.float32)\n",
    "\n",
    "# Construct the PyTorch dataset and data loaders\n",
    "torchDataset = CreatePytorchDataset(X, y)\n",
    "train_loader = torch.utils.data.DataLoader(torchDataset, batch_size=mini_batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torchDataset, batch_size=mini_batch_size, shuffle=True)\n",
    "\n",
    "# Construct the model and optimizer and train the model\n",
    "inputFeatures = X.shape[1]\n",
    "outputFeatures = y.shape[1]\n",
    "modelEncoder = ConstructModelEncoder(inputFeatures, 117)\n",
    "modelDecoder = ConstructModelDecoder(117, outputFeatures)\n",
    "\n",
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = torch.optim.Adam(list(modelEncoder.parameters()) + list(modelDecoder.parameters()), lr=lr)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx\n",
    "\n",
    "def ExportModelToOnnx(model, filename, dummy_input):\n",
    "    torch.onnx.export(model,                     # model being run\n",
    "                      dummy_input,               # model input (or a tuple for multiple inputs)\n",
    "                      filename,                  # where to save the model (can be a file or file-like object)\n",
    "                      export_params=True,        # store the trained parameter weights inside the model file\n",
    "                      opset_version=11,          # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                      input_names = ['input'],   # the model's input names\n",
    "                      output_names = ['output'], # the model's output names\n",
    "                      dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                    'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training(modelEncoder, modelDecoder, train_loader, test_loader, optimizer):\n",
    "\n",
    "    timeCode = datetime.now().strftime(\"%H:%M:%S\") # used for TensorBoard\n",
    "\n",
    "    # Re-train on the same data for the given amount of epochs\n",
    "    losses = []\n",
    "    lossesTesting = []\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "        print(\"Epoch \" + str(epoch) + \": \")\n",
    "        running_loss = 0.0\n",
    "        current_mini_batch = 0\n",
    "\n",
    "        # Mini-batches\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "            # Get the inputs and labels/outputs from the dataset\n",
    "            X, y = data\n",
    "\n",
    "            #################\n",
    "            # Backpropagation\n",
    "\n",
    "            # Zero the gradients\n",
    "            modelEncoder.zero_grad()\n",
    "            modelDecoder.zero_grad()\n",
    "\n",
    "            # Perform forward pass and compute prediction\n",
    "            pred_y = modelDecoder(modelEncoder(X))\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(pred_y, y)\n",
    "            running_loss += loss.item()\n",
    "            tensorBoardWriter.add_scalar(timeCode + \" Loss / train\", loss.item(), current_mini_batch)\n",
    "\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            print_each = 100\n",
    "            if current_mini_batch % print_each == print_each - 1:\n",
    "                averaged_loss = running_loss / print_each\n",
    "                print(\"Loss after mini-batch %5d: %.5f\" % (current_mini_batch + 1, averaged_loss))\n",
    "                tensorBoardWriter.add_scalar(timeCode + \" Running loss / train\", averaged_loss, current_mini_batch)\n",
    "                losses.append(averaged_loss)\n",
    "                running_loss = 0.0\n",
    "\n",
    "            current_mini_batch = current_mini_batch + 1\n",
    "\n",
    "        pred_y_testing = modelDecoder(modelEncoder(Xt))\n",
    "        total = 0.0\n",
    "        for i in range(len(pred_y_testing)):\n",
    "            loss_testing = criterion(pred_y_testing[i], yt[i])\n",
    "            total += loss_testing.item()\n",
    "        lossesTesting.append(total)\n",
    "        print(total)\n",
    "\n",
    "    lossData = pd.DataFrame(losses)\n",
    "    lossData.to_csv(\"D:/MMDatabaseAutoEncoderModelSpace1layer1872to117SplitModelOpset11LR1e-4Epoch100Batch128_WithValidationTest.csv\", index=False)\n",
    "    plt.plot(losses)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('mini-batch')\n",
    "    plt.title(\"Learning rate %f\"%(lr))\n",
    "    plt.show()\n",
    "\n",
    "    lossDataTesting = pd.DataFrame(lossesTesting)\n",
    "    lossDataTesting.to_csv(\"D:/MMDatabaseAutoEncoderModelSpace1layer1872to117SplitModelOpset11LR1e-4Epoch100Batch128_WithValidationTest_This.csv\", index=False)\n",
    "    plt.plot(lossesTesting)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('mini-batch')\n",
    "    plt.title(\"Learning rate %f\"%(lr))\n",
    "    plt.show()\n",
    "\n",
    "    # TensorBoard\n",
    "    tensorBoardWriter.close()\n",
    "\n",
    "Training(modelEncoder, modelDecoder, train_loader, test_loader, optimizer)\n",
    "\n",
    "# Export the trained model to an ONNX file\n",
    "filenameEncoder = folderPath + 'MMDatabaseAutoEncoderModelSpace1layer1872to117SplitModelOpset11LR1e-4Epoch100Batch128_WithValidationTest.onnx'\n",
    "ExportModelToOnnx(modelEncoder, filenameEncoder, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"D:/MMDatabaseAutoEncoderModelSpace1layer1872to117SplitModelOpset11LR1e-4Epoch100Batch128_WithValidationTest.onnx\")\n",
    "X = torch.tensor(dataPosRotModelSpaceScaledTesting, dtype=torch.float32)\n",
    "ort_input = {\"input\": X.numpy()}\n",
    "ort_outs = ort_session.run(None, ort_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalFeatures = pd.DataFrame(ort_outs[0])\n",
    "finalFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalFeatures.to_csv(\"D:/InferencedFeatures_MMDatabaseAutoEncoderModelSpace1layer1872to117SplitModelOpset11LR1e-4Epoch100Batch128_WithValidationTest_Testing.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64e2c40b1e2e5187dbc581262b84e05636cbc152dfd1c72af75c368d3284d05b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
